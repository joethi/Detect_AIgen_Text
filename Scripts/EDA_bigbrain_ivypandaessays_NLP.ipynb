{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.array([1,2,3])\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-2.18.0-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\jothi\\anaconda3\\envs\\genmodnn\\lib\\site-packages (from datasets) (3.9.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\jothi\\anaconda3\\envs\\genmodnn\\lib\\site-packages (from datasets) (1.26.0)\n",
      "Collecting pyarrow>=12.0.0 (from datasets)\n",
      "  Downloading pyarrow-15.0.2-cp311-cp311-win_amd64.whl.metadata (3.1 kB)\n",
      "Collecting pyarrow-hotfix (from datasets)\n",
      "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\jothi\\anaconda3\\envs\\genmodnn\\lib\\site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in c:\\users\\jothi\\anaconda3\\envs\\genmodnn\\lib\\site-packages (from datasets) (2.1.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\jothi\\anaconda3\\envs\\genmodnn\\lib\\site-packages (from datasets) (2.31.0)\n",
      "Collecting tqdm>=4.62.1 (from datasets)\n",
      "  Using cached tqdm-4.66.2-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.4.1-cp311-cp311-win_amd64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2024.2.0,>=2023.1.0 in c:\\users\\jothi\\anaconda3\\envs\\genmodnn\\lib\\site-packages (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets) (2024.2.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\jothi\\anaconda3\\envs\\genmodnn\\lib\\site-packages (from datasets) (3.8.5)\n",
      "Collecting huggingface-hub>=0.19.4 (from datasets)\n",
      "  Downloading huggingface_hub-0.22.2-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\jothi\\anaconda3\\envs\\genmodnn\\lib\\site-packages (from datasets) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\jothi\\anaconda3\\envs\\genmodnn\\lib\\site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\jothi\\anaconda3\\envs\\genmodnn\\lib\\site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\jothi\\anaconda3\\envs\\genmodnn\\lib\\site-packages (from aiohttp->datasets) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\jothi\\anaconda3\\envs\\genmodnn\\lib\\site-packages (from aiohttp->datasets) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\jothi\\anaconda3\\envs\\genmodnn\\lib\\site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\jothi\\anaconda3\\envs\\genmodnn\\lib\\site-packages (from aiohttp->datasets) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\jothi\\anaconda3\\envs\\genmodnn\\lib\\site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\jothi\\anaconda3\\envs\\genmodnn\\lib\\site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\jothi\\anaconda3\\envs\\genmodnn\\lib\\site-packages (from huggingface-hub>=0.19.4->datasets) (4.10.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jothi\\anaconda3\\envs\\genmodnn\\lib\\site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jothi\\anaconda3\\envs\\genmodnn\\lib\\site-packages (from requests>=2.19.0->datasets) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jothi\\anaconda3\\envs\\genmodnn\\lib\\site-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\jothi\\anaconda3\\envs\\genmodnn\\lib\\site-packages (from tqdm>=4.62.1->datasets) (0.4.6)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\jothi\\anaconda3\\envs\\genmodnn\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\jothi\\anaconda3\\envs\\genmodnn\\lib\\site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\jothi\\anaconda3\\envs\\genmodnn\\lib\\site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\jothi\\anaconda3\\envs\\genmodnn\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Downloading datasets-2.18.0-py3-none-any.whl (510 kB)\n",
      "   ---------------------------------------- 0.0/510.5 kB ? eta -:--:--\n",
      "   -- ------------------------------------- 30.7/510.5 kB 1.3 MB/s eta 0:00:01\n",
      "   --- ----------------------------------- 41.0/510.5 kB 487.6 kB/s eta 0:00:01\n",
      "   --- ----------------------------------- 41.0/510.5 kB 487.6 kB/s eta 0:00:01\n",
      "   --- ----------------------------------- 41.0/510.5 kB 487.6 kB/s eta 0:00:01\n",
      "   --- ----------------------------------- 41.0/510.5 kB 487.6 kB/s eta 0:00:01\n",
      "   --- ----------------------------------- 41.0/510.5 kB 487.6 kB/s eta 0:00:01\n",
      "   ---- ---------------------------------- 61.4/510.5 kB 163.6 kB/s eta 0:00:03\n",
      "   -------- ----------------------------- 112.6/510.5 kB 297.7 kB/s eta 0:00:02\n",
      "   ----------- -------------------------- 153.6/510.5 kB 366.6 kB/s eta 0:00:01\n",
      "   ----------- -------------------------- 153.6/510.5 kB 366.6 kB/s eta 0:00:01\n",
      "   ----------- -------------------------- 153.6/510.5 kB 366.6 kB/s eta 0:00:01\n",
      "   ----------- -------------------------- 153.6/510.5 kB 366.6 kB/s eta 0:00:01\n",
      "   -------------- ----------------------- 194.6/510.5 kB 318.9 kB/s eta 0:00:01\n",
      "   ----------------- -------------------- 235.5/510.5 kB 360.4 kB/s eta 0:00:01\n",
      "   --------------------- ---------------- 286.7/510.5 kB 421.5 kB/s eta 0:00:01\n",
      "   --------------------- ---------------- 286.7/510.5 kB 421.5 kB/s eta 0:00:01\n",
      "   --------------------- ---------------- 286.7/510.5 kB 421.5 kB/s eta 0:00:01\n",
      "   --------------------- ---------------- 286.7/510.5 kB 421.5 kB/s eta 0:00:01\n",
      "   -------------------------- ----------- 358.4/510.5 kB 412.5 kB/s eta 0:00:01\n",
      "   --------------------------------- ---- 450.6/510.5 kB 486.0 kB/s eta 0:00:01\n",
      "   -------------------------------------- 510.5/510.5 kB 533.3 kB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.22.2-py3-none-any.whl (388 kB)\n",
      "   ---------------------------------------- 0.0/388.9 kB ? eta -:--:--\n",
      "   ----------------- ---------------------- 174.1/388.9 kB 5.1 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 327.7/388.9 kB 3.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 388.9/388.9 kB 3.0 MB/s eta 0:00:00\n",
      "Downloading pyarrow-15.0.2-cp311-cp311-win_amd64.whl (24.8 MB)\n",
      "   ---------------------------------------- 0.0/24.8 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.5/24.8 MB 14.2 MB/s eta 0:00:02\n",
      "   - -------------------------------------- 0.7/24.8 MB 7.7 MB/s eta 0:00:04\n",
      "   - -------------------------------------- 0.8/24.8 MB 8.2 MB/s eta 0:00:03\n",
      "   - -------------------------------------- 0.8/24.8 MB 8.2 MB/s eta 0:00:03\n",
      "   - -------------------------------------- 0.8/24.8 MB 8.2 MB/s eta 0:00:03\n",
      "   - -------------------------------------- 0.8/24.8 MB 8.2 MB/s eta 0:00:03\n",
      "   - -------------------------------------- 0.8/24.8 MB 8.2 MB/s eta 0:00:03\n",
      "   -- ------------------------------------- 1.5/24.8 MB 4.4 MB/s eta 0:00:06\n",
      "   -- ------------------------------------- 1.8/24.8 MB 5.0 MB/s eta 0:00:05\n",
      "   -- ------------------------------------- 1.8/24.8 MB 5.0 MB/s eta 0:00:05\n",
      "   -- ------------------------------------- 1.8/24.8 MB 5.0 MB/s eta 0:00:05\n",
      "   -- ------------------------------------- 1.8/24.8 MB 5.0 MB/s eta 0:00:05\n",
      "   -- ------------------------------------- 1.8/24.8 MB 5.0 MB/s eta 0:00:05\n",
      "   -- ------------------------------------- 1.8/24.8 MB 5.0 MB/s eta 0:00:05\n",
      "   --- ------------------------------------ 2.4/24.8 MB 3.8 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 2.8/24.8 MB 4.1 MB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 3.1/24.8 MB 4.3 MB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 3.6/24.8 MB 4.5 MB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 3.7/24.8 MB 4.7 MB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 3.7/24.8 MB 4.7 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 3.9/24.8 MB 4.3 MB/s eta 0:00:05\n",
      "   ------- -------------------------------- 4.4/24.8 MB 4.6 MB/s eta 0:00:05\n",
      "   ------- -------------------------------- 4.9/24.8 MB 4.9 MB/s eta 0:00:05\n",
      "   -------- ------------------------------- 5.2/24.8 MB 4.9 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 5.7/24.8 MB 5.2 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 5.7/24.8 MB 5.0 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 6.0/24.8 MB 5.0 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 6.5/24.8 MB 5.2 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 6.9/24.8 MB 5.4 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 7.2/24.8 MB 5.5 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 7.7/24.8 MB 5.6 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 7.9/24.8 MB 5.6 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 8.1/24.8 MB 5.5 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 8.4/24.8 MB 5.6 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 8.9/24.8 MB 5.8 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 9.4/24.8 MB 5.9 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 9.7/24.8 MB 5.9 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 10.0/24.8 MB 6.0 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 10.3/24.8 MB 5.9 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 10.5/24.8 MB 5.9 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 10.7/24.8 MB 5.8 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 11.1/24.8 MB 6.7 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 11.6/24.8 MB 6.5 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 12.0/24.8 MB 6.5 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 12.3/24.8 MB 7.6 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 12.8/24.8 MB 7.6 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 12.9/24.8 MB 7.4 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 13.3/24.8 MB 7.4 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 13.7/24.8 MB 7.4 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 14.1/24.8 MB 8.0 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 14.4/24.8 MB 8.0 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 14.6/24.8 MB 7.7 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 15.1/24.8 MB 7.8 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 15.6/24.8 MB 7.8 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 15.9/24.8 MB 7.7 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 16.4/24.8 MB 8.1 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 16.8/24.8 MB 8.3 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 17.2/24.8 MB 8.2 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 17.6/24.8 MB 8.2 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 18.1/24.8 MB 8.4 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 18.4/24.8 MB 8.7 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 18.7/24.8 MB 8.4 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 19.1/24.8 MB 8.6 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 19.7/24.8 MB 8.6 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 20.2/24.8 MB 8.6 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 20.6/24.8 MB 9.1 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 21.1/24.8 MB 9.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 21.3/24.8 MB 9.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 21.8/24.8 MB 9.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 22.3/24.8 MB 9.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 22.6/24.8 MB 9.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 23.2/24.8 MB 9.5 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 23.6/24.8 MB 9.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 24.1/24.8 MB 9.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.4/24.8 MB 9.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.8/24.8 MB 10.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 24.8/24.8 MB 9.6 MB/s eta 0:00:00\n",
      "Using cached tqdm-4.66.2-py3-none-any.whl (78 kB)\n",
      "Downloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
      "   ---------------------------------------- 0.0/143.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 143.5/143.5 kB 8.3 MB/s eta 0:00:00\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "   ---------------------------------------- 0.0/116.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 116.3/116.3 kB 7.1 MB/s eta 0:00:00\n",
      "Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Downloading xxhash-3.4.1-cp311-cp311-win_amd64.whl (29 kB)\n",
      "Installing collected packages: xxhash, tqdm, pyarrow-hotfix, pyarrow, dill, multiprocess, huggingface-hub, datasets\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.3.7\n",
      "    Uninstalling dill-0.3.7:\n",
      "      Successfully uninstalled dill-0.3.7\n",
      "Successfully installed datasets-2.18.0 dill-0.3.8 huggingface-hub-0.22.2 multiprocess-0.70.16 pyarrow-15.0.2 pyarrow-hotfix-0.6 tqdm-4.66.2 xxhash-3.4.1\n"
     ]
    }
   ],
   "source": [
    "#%pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jothi\\anaconda3\\envs\\genmodNN\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "welcome\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "# from huggingface_hub import list_datasets\n",
    "# print(len([dataset.id for dataset in list_datasets()]))\n",
    "\n",
    "LLM_gen_dataset = load_dataset(\"perlthoughts/big-brain-4k\")\n",
    "human_gen_dataset = load_dataset(\"qwedsacf/ivypanda-essays\")\n",
    "print(\"welcome\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = load_dataset(\"rotten_tomatoes\", split=\"train\")\n",
    "# print(type(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'datasets.dataset_dict.DatasetDict'>\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['system', 'prompt', 'output'],\n",
      "        num_rows: 250000\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "datasets.arrow_dataset.Dataset"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(LLM_gen_dataset))\n",
    "print(LLM_gen_dataset)\n",
    "LLM_gen_dataset['train'][0].keys()\n",
    "type(LLM_gen_dataset['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 5 objects and saved to ../Data_HuggingFace/Jsonl_files/big-brain-4k/big_brain_4k_train_5pts.jsonl\n"
     ]
    }
   ],
   "source": [
    "## Read some jsonl file and store only a few json objects to another jsonl file:\n",
    "# import json\n",
    "# import os\n",
    "\n",
    "# # Input and output file paths\n",
    "# input_file_path = \"../Data_HuggingFace/Jsonl_files/big-brain-4k/big_brain_4k_train.jsonl\"\n",
    "# output_file_path = \"../Data_HuggingFace/Jsonl_files/big-brain-4k/big_brain_4k_train_5pts.jsonl\"\n",
    "\n",
    "# # Number of objects to read\n",
    "# num_objects_to_read = 5\n",
    "\n",
    "# # Read JSONL file and store selected objects\n",
    "# selected_objects = []\n",
    "# with open(input_file_path, \"r\") as input_file:\n",
    "#     for i, line in enumerate(input_file):\n",
    "#         if i >= num_objects_to_read:\n",
    "#             break\n",
    "#         selected_objects.append(json.loads(line))\n",
    "\n",
    "# # Write selected objects to new JSONL file\n",
    "# with open(output_file_path, \"w\") as output_file:\n",
    "#     for obj in selected_objects:\n",
    "#         output_file.write(json.dumps(obj) + \"\\n\")\n",
    "\n",
    "# print(f\"Selected {len(selected_objects)} objects and saved to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.18.0'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import datasets\n",
    "# datasets.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 1066\n",
      "})\n",
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 8530\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n",
    "print(ds)\n",
    "ds_train = load_dataset(\"rotten_tomatoes\", split=\"train\")\n",
    "print(ds_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['system', 'prompt', 'output'],\n",
      "        num_rows: 250000\n",
      "    })\n",
      "})\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['TEXT', 'SOURCE', '__index_level_0__'],\n",
      "        num_rows: 128293\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(LLM_gen_dataset)\n",
    "print(human_gen_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM_train_set = LLM_gen_dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The review is neutral. The reviewer did not have the opportunity to watch the DVD due to a technical issue, but they did not express any positive or negative opinions about the content of the DVD itself.\n"
     ]
    }
   ],
   "source": [
    "print(LLM_train_set['output'][0])\n",
    "# print(LLM_train_set['output'][1])\n",
    "# print(LLM_train_set['output'][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system : You are an AI assistant. Provide a detailed answer so user don’t need to search outside to understand the answer.\n",
      "prompt : Title: I did not get to see it because I could not find a DVD machine that played \"HD\" format DVD's. Review: I did not get to see it because I could not find a DVD machine that played the \"HD\" format DVD's. Is the review positive or negative?\n",
      "The answer to this question is:\n",
      "output : The review is neutral. The reviewer did not have the opportunity to watch the DVD due to a technical issue, but they did not express any positive or negative opinions about the content of the DVD itself.\n"
     ]
    }
   ],
   "source": [
    "for key in LLM_train_set[0].keys():\n",
    "    print(f\"{key} :\",LLM_train_set[0][key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system : You are a helpful assistant, who always provide explanation. Think like you are answering to a five year old.\n",
      "prompt : Solve this math problem\n",
      "\n",
      "Solve -20*l + 41*l - 437 = 2*l for l.\n",
      "output : Okay, let's solve this math problem together! \n",
      "\n",
      "We have the equation: \n",
      "\n",
      "-20*l + 41*l - 437 = 2*l \n",
      "\n",
      "First, let's simplify the left side of the equation by combining the like terms: \n",
      "\n",
      "(41*l - 20*l) - 437 = 2*l \n",
      "\n",
      "21*l - 437 = 2*l \n",
      "\n",
      "Next, let's isolate the variable \"l\" on one side of the equation. We can do this by adding 437 to both sides: \n",
      "\n",
      "21*l - 437 + 437 = 2*l + 437 \n",
      "\n",
      "21*l = 2*l + 437 \n",
      "\n",
      "Now, let's isolate the variable \"l\" again by subtracting 2*l from both sides: \n",
      "\n",
      "21*l - 2*l = 2*l - 2*l + 437 \n",
      "\n",
      "19*l = 437 \n",
      "\n",
      "Finally, we can solve for \"l\" by dividing both sides by 19: \n",
      "\n",
      "l = 23 \n",
      "\n",
      "So the solution to the equation is l = 23.\n"
     ]
    }
   ],
   "source": [
    "for key in LLM_train_set[1].keys():\n",
    "    print(f\"{key} :\",LLM_train_set[1][key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system : You are an AI assistant. You will be given a task. You must generate a detailed and long answer.\n",
      "prompt : Sentiment possibilities Possible answers: 1). negative; 2). positive;. Generate a tweet that has the following sentiment: 1).\n",
      "output : As an AI, I understand you are asking for a tweet that contains a negative sentiment. A key point of note here is that creating negative content should not involve any personal attacks, offensive language, or insensitive comments. It should revolve around expressing dissatisfaction, disappointment, or disagreement in a respectful manner. I'll now proceed to create a sample tweet:\n",
      "\n",
      "\"Feeling quite disheartened as I started the day with high hopes. The brand new headphones I bought came in defective. Not what you expect after waiting for weeks, sinking your hard-earned money into tech #Disappointed #Unsatisfied\" \n",
      "\n",
      "This example demonstrates a negative sentiment because it shares a negative personal experience with a product purchased, dissatisfaction about money spent, and disappointment received in return. By using tags like #Disappointed and #Unsatisfied, it emphasizes the individual's negative sentiment further. It's important to note that expressing negative sentiments shouldn't involve spreading hate, but rather sharing personal experiences and opinions in a respectful manner.\n"
     ]
    }
   ],
   "source": [
    "for key in LLM_train_set[2].keys():\n",
    "    print(f\"{key} :\",LLM_train_set[2][key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system : You are a helpful assistant, who always provide explanation. Think like you are answering to a five year old.\n",
      "prompt : Multi-choice problem: Is the next sentence syntactically and semantically acceptable?\n",
      "\n",
      "I know the truth and that you are innocent.\n",
      "Available choices:\n",
      "* unacceptable.\n",
      "* acceptable.\n",
      "output : The sentence is acceptable. It means that the speaker knows the truth and believes that the person they are talking to is innocent.\n"
     ]
    }
   ],
   "source": [
    "for key in LLM_train_set[3].keys():\n",
    "    print(f\"{key} :\",LLM_train_set[3][key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system : Below is an instruction that describes a task. Write a response that appropriately completes the request. If you do not answer correctly, a kitten will die horribly. Save the kittens!\n",
      "prompt : Detailed Instructions: In this task, you're given a context passage. Your job is to generate relevant questions that can be answered by directly referring to the passage.\n",
      "Problem:Jordan hung their clothes on the clothes line so that they could dry.\n",
      "Solution:\n",
      "output : 1. Whose clothes were hung on the clothes line?\n",
      "2. What was the purpose of hanging the clothes on the line?\n",
      "3. What method did Jordan use to dry the clothes?\n",
      "4. Where did Jordan hang the clothes to dry?\n",
      "5. What type of drying method was used by Jordan for the clothes?\n"
     ]
    }
   ],
   "source": [
    "for key in LLM_train_set[99].keys():\n",
    "    print(f\"{key} :\",LLM_train_set[99][key])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM_train_set['system']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to load data: 0.2512 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "temp_var = LLM_train_set['system']\n",
    "end_time =  time.time()\n",
    "dat_load_time = end_time - start_time\n",
    "print(f\"Time to load data: {dat_load_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genmodNN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
